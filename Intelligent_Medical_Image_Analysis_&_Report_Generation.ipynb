{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti0J5DVcDkUl"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from skimage import exposure, restoration\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7dqPrCHDrsB"
      },
      "outputs": [],
      "source": [
        "# Define dataset path\n",
        "dataset_path = \"/content/drive/MyDrive/University /Spring 2025/Data Mining and Machine Learning /Project/GAN-Traning Images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f92FmJkCDruh"
      },
      "outputs": [],
      "source": [
        "# Check if path exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(\"✅ Dataset path exists.\")\n",
        "    print(\"Contents of dataset folder:\", os.listdir(dataset_path))\n",
        "else:\n",
        "    print(\"❌ Dataset path does not exist. Check the path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5sL-OwDvIR6d"
      },
      "outputs": [],
      "source": [
        "# Get sample images\n",
        "image_files = [f for f in os.listdir(dataset_path) if f.endswith(('.jpg', '.png'))][:6]\n",
        "image_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BfQWjLFDrw6"
      },
      "outputs": [],
      "source": [
        "# Get sample images for initial visualization (grayscale)\n",
        "image_files = [f for f in os.listdir(dataset_path) if f.endswith(('.jpg', '.png'))]\n",
        "image_files\n",
        "\n",
        "# Plot initial sample images (grayscale)\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "num_images_to_plot = min(6, len(image_files)) # Ensure we don't try to plot more than 6 images\n",
        "for i in range(num_images_to_plot):\n",
        "    img_name = image_files[i]\n",
        "    img = Image.open(os.path.join(dataset_path, img_name)).convert(\"L\") # Convert to grayscale [1]\n",
        "    ax = axes[i // 3, i % 3]\n",
        "    ax.imshow(img, cmap=\"gray\")\n",
        "    ax.set_title(img_name)\n",
        "    ax.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSFL3JgiHGLS"
      },
      "outputs": [],
      "source": [
        "# 1. Inspect dataset structure\n",
        "# We assume the dataset has a structure like: dataset_path/<class_name>/image_files...\n",
        "classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "print(\"Detected classes:\", classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSAgfWffDrzP"
      },
      "outputs": [],
      "source": [
        "# Inspect dataset structure and get class information\n",
        "classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "print(\"Detected classes:\", classes)\n",
        "# Get list of image paths and their labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Assuming your dataset is structured like: dataset_path/class_name/image.jpg [5, 6]\n",
        "for cls in classes:\n",
        "    cls_folder = os.path.join(dataset_path, cls)\n",
        "    imgs = glob(os.path.join(cls_folder, '*.[jp][pn]g')) # jpg, jpeg, png files [5]\n",
        "    image_paths.extend(imgs)\n",
        "    labels.extend([cls]*len(imgs))\n",
        "print(\"Total images loaded:\", len(image_paths))\n",
        "print(f\"Labels: {labels[:5]}\") # Print first 5 labels [7]\n",
        "print(\"Class distribution:\", Counter(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BdsdBmSHSH8"
      },
      "outputs": [],
      "source": [
        "# Get list of image paths and their labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "for cls in classes:\n",
        "    cls_folder = os.path.join(dataset_path, cls)\n",
        "    # Assuming common image file extensions; adjust if necessary.\n",
        "    imgs = glob(os.path.join(cls_folder, '*.[jp][pn]g'))  # jpg, jpeg, png files\n",
        "    image_paths.extend(imgs)\n",
        "    labels.extend([cls]*len(imgs))\n",
        "\n",
        "print(\"Total images loaded:\", len(image_paths))\n",
        "print(\"Class distribution:\", Counter(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES8uYm_FJAB4"
      },
      "outputs": [],
      "source": [
        "# List all image files (assuming jpg and png images)\n",
        "image_paths = glob(os.path.join(dataset_path, \"*.[jp][pn]g\"))\n",
        "print(\"Total images found:\", len(image_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI9jjS12HSKi"
      },
      "outputs": [],
      "source": [
        "# 2. Load and visualize sample images\n",
        "def load_image(image_path):\n",
        "    # Read the image in color first (for visualization)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error loading {image_path}\")\n",
        "    # Convert from BGR (OpenCV default) to RGB\n",
        "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuBeKsRaDr1d"
      },
      "outputs": [],
      "source": [
        "# Function to load images in color (BGR to RGB conversion for visualization)\n",
        "def load_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error loading {image_path}\")\n",
        "        return None\n",
        "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc9RdJ2NDr3y"
      },
      "outputs": [],
      "source": [
        "# Visualize a few sample images\n",
        "num_samples = 6  # Number of images to display (you can adjust this)\n",
        "\n",
        "if len(image_paths) > 0:\n",
        "    # Create subplots (adjusting layout based on number of samples)\n",
        "    cols = 3\n",
        "    rows = (num_samples + cols - 1) // cols  # Calculate required rows\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
        "\n",
        "    # Flatten axes for easy iteration if it's a 2D array\n",
        "    axes = axes.flatten() if isinstance(axes, (list, np.ndarray)) else [axes]\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if i < len(image_paths):\n",
        "            img = load_image(image_paths[i])\n",
        "            if img is not None:\n",
        "                axes[i].imshow(img)\n",
        "                axes[i].axis('off')\n",
        "                axes[i].set_title(f\"Image {i+1}\")\n",
        "        else:\n",
        "            # Hide unused subplots\n",
        "            axes[i].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No images found in the dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgG4I40iDr6I"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function\n",
        "def preprocess_image(image_path):\n",
        "    # Load image in grayscale\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # Check if image is loaded\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Image at {image_path} cannot be loaded.\")\n",
        "    # Intensity normalization (contrast stretching) [12]\n",
        "    p2, p98 = np.percentile(img, (2, 98))\n",
        "    img_norm = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
        "    # Noise reduction using Gaussian filtering [12]\n",
        "    img_denoised = cv2.GaussianBlur(img_norm, (5, 5), 0)\n",
        "    # ROI extraction (placeholder) [12]\n",
        "    roi = img_denoised\n",
        "    return roi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnIFvrX6Dr8O"
      },
      "outputs": [],
      "source": [
        "# 3. Preprocessing functions\n",
        "def preprocess_image(image_path):\n",
        "    # Load image in grayscale\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if image is loaded\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Image at {image_path} cannot be loaded.\")\n",
        "\n",
        "    # Intensity normalization (contrast stretching)\n",
        "    p2, p98 = np.percentile(img, (2, 98))\n",
        "    img_norm = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
        "\n",
        "    # Noise reduction using Gaussian filtering\n",
        "    img_denoised = cv2.GaussianBlur(img_norm, (5, 5), 0)\n",
        "\n",
        "    # ROI extraction: as an example, here we simply return the full image.\n",
        "    # You might add additional ROI extraction logic based on your application.\n",
        "    roi = img_denoised\n",
        "\n",
        "    return roi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdbOUuIMDsBD"
      },
      "outputs": [],
      "source": [
        "# Process a sample image and visualize before & after preprocessing\n",
        "if image_paths:  # Check if image_paths is not empty\n",
        "    sample_path = image_paths[0]\n",
        "    original_img = cv2.imread(sample_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if original_img is not None:\n",
        "        processed_img = preprocess_image(sample_path)  # Assuming you have defined this function\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        axes[0].imshow(original_img, cmap='gray')\n",
        "        axes[0].set_title(\"Original (Grayscale)\")\n",
        "        axes[0].axis('off')\n",
        "        axes[1].imshow(processed_img, cmap='gray')\n",
        "        axes[1].set_title(\"Preprocessed\")\n",
        "        axes[1].axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Error: Could not load image at {sample_path}\")\n",
        "else:\n",
        "    print(\"No images found in the dataset. Skipping preprocessing visualization.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sur3P1w8DsDB"
      },
      "outputs": [],
      "source": [
        "# Process a sample image and visualize before & after preprocessing\n",
        "if image_paths:  # Check if image_paths is not empty\n",
        "    sample_path = image_paths[0]\n",
        "    original_img = cv2.imread(sample_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if original_img is not None:\n",
        "        processed_img = preprocess_image(sample_path)  # Assuming you have defined this function\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        axes[0].imshow(original_img, cmap='gray')\n",
        "        axes[0].set_title(\"Original (Grayscale)\")\n",
        "        axes[0].axis('off')\n",
        "        axes[1].imshow(processed_img, cmap='gray')\n",
        "        axes[1].set_title(\"Preprocessed\")\n",
        "        axes[1].axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Error: Could not load image at {sample_path}\")\n",
        "else:\n",
        "    print(\"No images found in the dataset. Skipping preprocessing visualization.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ia3PwfGDsFV"
      },
      "outputs": [],
      "source": [
        "#Phase 2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from skimage.feature import graycomatrix, graycoprops # Changed 'greycomatrix' to 'graycomatrix' and 'greycoprops' to 'graycoprops'\n",
        "from skimage.measure import shannon_entropy\n",
        "from scipy.stats import skew, kurtosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0UyE_MZqr9q"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic grayscale image data (simulating MRI images)\n",
        "def generate_synthetic_images(num_images=100, img_size=(128, 128)):\n",
        "    return [np.random.randint(0, 256, img_size, dtype=np.uint8) for _ in range(num_images)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXkRBvKXqsAI"
      },
      "outputs": [],
      "source": [
        "# Statistical Analysis\n",
        "def compute_statistics(images):\n",
        "    stats = []\n",
        "    for img in images:\n",
        "        mean_val = np.mean(img)\n",
        "        var_val = np.var(img)\n",
        "        entropy = shannon_entropy(img)\n",
        "        skewness = skew(img.flatten())\n",
        "        kurt = kurtosis(img.flatten())\n",
        "        stats.append([mean_val, var_val, entropy, skewness, kurt])\n",
        "    return np.array(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCX323txqsCv"
      },
      "outputs": [],
      "source": [
        "# Visualization - PCA & t-SNE\n",
        "def visualize_dim_reduction(features):\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_pca = pca.fit_transform(features)\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    reduced_tsne = tsne.fit_transform(features)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    axes[0].scatter(reduced_pca[:, 0], reduced_pca[:, 1], alpha=0.7)\n",
        "    axes[0].set_title(\"PCA Projection\")\n",
        "\n",
        "    axes[1].scatter(reduced_tsne[:, 0], reduced_tsne[:, 1], alpha=0.7)\n",
        "    axes[1].set_title(\"t-SNE Projection\")\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVUs-NXCqsHV"
      },
      "outputs": [],
      "source": [
        "# Feature Extraction\n",
        "def extract_texture_features(images):\n",
        "    features = []\n",
        "    for img in images:\n",
        "        # Changed 'greycomatrix' to 'graycomatrix'\n",
        "        glcm = graycomatrix(img, [1], [0], symmetric=True, normed=True)\n",
        "        # Changed 'greycoprops' to 'graycoprops'\n",
        "        contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
        "        energy = graycoprops(glcm, 'energy')[0, 0]\n",
        "        features.append([contrast, energy])\n",
        "    return np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O21stHyXqsJg"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "images = generate_synthetic_images()\n",
        "stats = compute_statistics(images)\n",
        "texture_features = extract_texture_features(images)\n",
        "all_features = np.hstack((stats, texture_features))\n",
        "# Save the features to 'features.npy'\n",
        "np.save(\"features.npy\", all_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "LEArxUZ8qsL5",
        "outputId": "532fb48b-62a3-44ce-e9b7-e6ba7c80ae8f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualizations\n",
        "sns.heatmap(np.corrcoef(all_features.T), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "visualize_dim_reduction(all_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToT_WTn08ClS"
      },
      "outputs": [],
      "source": [
        "#Phase 3\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztpeqvgp8FG9",
        "outputId": "43417957-ae5c-458d-ffd0-cd114d16f294"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "if os.path.exists(\"features.npy\") and os.path.exists(\"labels.npy\"):\n",
        "    X_features = np.load(\"features.npy\")\n",
        "    y_labels = np.load(\"labels.npy\")  # Attempting to load y_labels\n",
        "    print(\"✅ Features and labels loaded successfully.\")\n",
        "else:\n",
        "    # If 'labels.npy' doesn't exist, create it or load it from another source\n",
        "    # For example, you might have your labels in a list or another file\n",
        "    # In this example, I'll create a sample labels array:\n",
        "    y_labels = np.random.randint(0, 2, size=X_features.shape[0])  # Example: binary labels (0 or 1)\n",
        "    np.save(\"labels.npy\", y_labels)  # Save to 'labels.npy' for future use\n",
        "    print(\"⚠️ labels.npy not found. Created sample labels and saved to file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGzRnrXH8FJs"
      },
      "outputs": [],
      "source": [
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI-M7tR8HpQ6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from skimage import exposure, restoration\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nnTzbK2HpQ8"
      },
      "outputs": [],
      "source": [
        "# Define dataset path\n",
        "dataset_path = \"/content/drive/MyDrive/University /Spring 2025/Data Mining and Machine Learning /Project/GAN-Traning Images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIYQ2PbiHpQ8",
        "outputId": "1e756cf1-dc41-4c46-c075-678f91638a2f"
      },
      "outputs": [],
      "source": [
        "# Check if path exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(\"✅ Dataset path exists.\")\n",
        "    print(\"Contents of dataset folder:\", os.listdir(dataset_path))\n",
        "else:\n",
        "    print(\"❌ Dataset path does not exist. Check the path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MXQpPElPHpQ9",
        "outputId": "bfa83092-20bb-4f60-9ad0-81a2197e04f7"
      },
      "outputs": [],
      "source": [
        "# Get sample images\n",
        "image_files = [f for f in os.listdir(dataset_path) if f.endswith(('.jpg', '.png'))][:6]\n",
        "image_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "VByYZmVPHpQ9",
        "outputId": "ca980c9b-e83d-4c21-9e3e-abe9dc8bf7f3"
      },
      "outputs": [],
      "source": [
        "# Get sample images for initial visualization (grayscale)\n",
        "image_files = [f for f in os.listdir(dataset_path) if f.endswith(('.jpg', '.png'))]\n",
        "image_files\n",
        "\n",
        "# Plot initial sample images (grayscale)\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "num_images_to_plot = min(6, len(image_files)) # Ensure we don't try to plot more than 6 images\n",
        "for i in range(num_images_to_plot):\n",
        "    img_name = image_files[i]\n",
        "    img = Image.open(os.path.join(dataset_path, img_name)).convert(\"L\") # Convert to grayscale [1]\n",
        "    ax = axes[i // 3, i % 3]\n",
        "    ax.imshow(img, cmap=\"gray\")\n",
        "    ax.set_title(img_name)\n",
        "    ax.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWW531CmHpQ9",
        "outputId": "e0f452a2-2fd3-4d13-acfa-5f739fd88c79"
      },
      "outputs": [],
      "source": [
        "# 1. Inspect dataset structure\n",
        "# We assume the dataset has a structure like: dataset_path/<class_name>/image_files...\n",
        "classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "print(\"Detected classes:\", classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq_izOPzHpQ9",
        "outputId": "81c22dd0-2cdd-490b-dac0-e1130451184a"
      },
      "outputs": [],
      "source": [
        "# Inspect dataset structure and get class information\n",
        "classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "print(\"Detected classes:\", classes)\n",
        "# Get list of image paths and their labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Assuming your dataset is structured like: dataset_path/class_name/image.jpg [5, 6]\n",
        "for cls in classes:\n",
        "    cls_folder = os.path.join(dataset_path, cls)\n",
        "    imgs = glob(os.path.join(cls_folder, '*.[jp][pn]g')) # jpg, jpeg, png files [5]\n",
        "    image_paths.extend(imgs)\n",
        "    labels.extend([cls]*len(imgs))\n",
        "print(\"Total images loaded:\", len(image_paths))\n",
        "print(f\"Labels: {labels[:5]}\") # Print first 5 labels [7]\n",
        "print(\"Class distribution:\", Counter(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEx0OKUyHpQ9",
        "outputId": "b9877aea-50c9-4ba4-99eb-b0f9dfbf3890"
      },
      "outputs": [],
      "source": [
        "# Get list of image paths and their labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "for cls in classes:\n",
        "    cls_folder = os.path.join(dataset_path, cls)\n",
        "    # Assuming common image file extensions; adjust if necessary.\n",
        "    imgs = glob(os.path.join(cls_folder, '*.[jp][pn]g'))  # jpg, jpeg, png files\n",
        "    image_paths.extend(imgs)\n",
        "    labels.extend([cls]*len(imgs))\n",
        "\n",
        "print(\"Total images loaded:\", len(image_paths))\n",
        "print(\"Class distribution:\", Counter(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OkFFC9uHpQ9",
        "outputId": "69812927-f322-4639-887a-162b2aa3434d"
      },
      "outputs": [],
      "source": [
        "# List all image files (assuming jpg and png images)\n",
        "image_paths = glob(os.path.join(dataset_path, \"*.[jp][pn]g\"))\n",
        "print(\"Total images found:\", len(image_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbhxisXnHpQ9"
      },
      "outputs": [],
      "source": [
        "# 2. Load and visualize sample images\n",
        "def load_image(image_path):\n",
        "    # Read the image in color first (for visualization)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error loading {image_path}\")\n",
        "    # Convert from BGR (OpenCV default) to RGB\n",
        "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIDMJg9eHpQ9"
      },
      "outputs": [],
      "source": [
        "# Function to load images in color (BGR to RGB conversion for visualization)\n",
        "def load_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error loading {image_path}\")\n",
        "        return None\n",
        "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j1YyCOiuHpQ9",
        "outputId": "a50cf786-441c-4b6e-e67c-c9d46dbc1806"
      },
      "outputs": [],
      "source": [
        "# Visualize a few sample images\n",
        "num_samples = 6  # Number of images to display (you can adjust this)\n",
        "\n",
        "if len(image_paths) > 0:\n",
        "    # Create subplots (adjusting layout based on number of samples)\n",
        "    cols = 3\n",
        "    rows = (num_samples + cols - 1) // cols  # Calculate required rows\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
        "\n",
        "    # Flatten axes for easy iteration if it's a 2D array\n",
        "    axes = axes.flatten() if isinstance(axes, (list, np.ndarray)) else [axes]\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if i < len(image_paths):\n",
        "            img = load_image(image_paths[i])\n",
        "            if img is not None:\n",
        "                axes[i].imshow(img)\n",
        "                axes[i].axis('off')\n",
        "                axes[i].set_title(f\"Image {i+1}\")\n",
        "        else:\n",
        "            # Hide unused subplots\n",
        "            axes[i].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No images found in the dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de07hPW4HpQ9"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Preprocessing function\n",
        "def preprocess_image(image_path):\n",
        "    # Load image in grayscale\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # Check if image is loaded\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Image at {image_path} cannot be loaded.\")\n",
        "    # Intensity normalization (contrast stretching) [12]\n",
        "    p2, p98 = np.percentile(img, (2, 98))\n",
        "    img_norm = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
        "    # Noise reduction using Gaussian filtering [12]\n",
        "    img_denoised = cv2.GaussianBlur(img_norm, (5, 5), 0)\n",
        "    # ROI extraction (placeholder) [12]\n",
        "    roi = img_denoised\n",
        "    return roi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irmpMHfGHpQ-"
      },
      "outputs": [],
      "source": [
        "# 3. Preprocessing functions\n",
        "def preprocess_image(image_path):\n",
        "    # Load image in grayscale\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if image is loaded\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Image at {image_path} cannot be loaded.\")\n",
        "\n",
        "    # Intensity normalization (contrast stretching)\n",
        "    p2, p98 = np.percentile(img, (2, 98))\n",
        "    img_norm = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
        "\n",
        "    # Noise reduction using Gaussian filtering\n",
        "    img_denoised = cv2.GaussianBlur(img_norm, (5, 5), 0)\n",
        "\n",
        "    # ROI extraction: as an example, here we simply return the full image.\n",
        "    # You might add additional ROI extraction logic based on your application.\n",
        "    roi = img_denoised\n",
        "\n",
        "    return roi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "oHhlhT0HHpQ-",
        "outputId": "aa345a99-08ae-4449-c484-dab9220f0a56"
      },
      "outputs": [],
      "source": [
        "# Process a sample image and visualize before & after preprocessing\n",
        "if image_paths:  # Check if image_paths is not empty\n",
        "    sample_path = image_paths[0]\n",
        "    original_img = cv2.imread(sample_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if original_img is not None:\n",
        "        processed_img = preprocess_image(sample_path)  # Assuming you have defined this function\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        axes[0].imshow(original_img, cmap='gray')\n",
        "        axes[0].set_title(\"Original (Grayscale)\")\n",
        "        axes[0].axis('off')\n",
        "        axes[1].imshow(processed_img, cmap='gray')\n",
        "        axes[1].set_title(\"Preprocessed\")\n",
        "        axes[1].axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Error: Could not load image at {sample_path}\")\n",
        "else:\n",
        "    print(\"No images found in the dataset. Skipping preprocessing visualization.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "4nhL-wUMHpQ-",
        "outputId": "86a47cba-b841-4725-dda9-4542845b30a3"
      },
      "outputs": [],
      "source": [
        "# Process a sample image and visualize before & after preprocessing\n",
        "if image_paths:  # Check if image_paths is not empty\n",
        "    sample_path = image_paths[0]\n",
        "    original_img = cv2.imread(sample_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if original_img is not None:\n",
        "        processed_img = preprocess_image(sample_path)  # Assuming you have defined this function\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        axes[0].imshow(original_img, cmap='gray')\n",
        "        axes[0].set_title(\"Original (Grayscale)\")\n",
        "        axes[0].axis('off')\n",
        "        axes[1].imshow(processed_img, cmap='gray')\n",
        "        axes[1].set_title(\"Preprocessed\")\n",
        "        axes[1].axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Error: Could not load image at {sample_path}\")\n",
        "else:\n",
        "    print(\"No images found in the dataset. Skipping preprocessing visualization.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIXq80mrHpQ-"
      },
      "outputs": [],
      "source": [
        "#Phase 2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from skimage.feature import graycomatrix, graycoprops # Changed 'greycomatrix' to 'graycomatrix' and 'greycoprops' to 'graycoprops'\n",
        "from skimage.measure import shannon_entropy\n",
        "from scipy.stats import skew, kurtosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ihgoMmnHpQ-"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic grayscale image data (simulating MRI images)\n",
        "def generate_synthetic_images(num_images=100, img_size=(128, 128)):\n",
        "    return [np.random.randint(0, 256, img_size, dtype=np.uint8) for _ in range(num_images)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn9d4_PwHpQ-"
      },
      "outputs": [],
      "source": [
        "# Statistical Analysis\n",
        "def compute_statistics(images):\n",
        "    stats = []\n",
        "    for img in images:\n",
        "        mean_val = np.mean(img)\n",
        "        var_val = np.var(img)\n",
        "        entropy = shannon_entropy(img)\n",
        "        skewness = skew(img.flatten())\n",
        "        kurt = kurtosis(img.flatten())\n",
        "        stats.append([mean_val, var_val, entropy, skewness, kurt])\n",
        "    return np.array(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZxklUtAHpQ-"
      },
      "outputs": [],
      "source": [
        "# Visualization - PCA & t-SNE\n",
        "def visualize_dim_reduction(features):\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_pca = pca.fit_transform(features)\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    reduced_tsne = tsne.fit_transform(features)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    axes[0].scatter(reduced_pca[:, 0], reduced_pca[:, 1], alpha=0.7)\n",
        "    axes[0].set_title(\"PCA Projection\")\n",
        "\n",
        "    axes[1].scatter(reduced_tsne[:, 0], reduced_tsne[:, 1], alpha=0.7)\n",
        "    axes[1].set_title(\"t-SNE Projection\")\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQSMdmfeHpQ-"
      },
      "outputs": [],
      "source": [
        "# Feature Extraction\n",
        "def extract_texture_features(images):\n",
        "    features = []\n",
        "    for img in images:\n",
        "        # Changed 'greycomatrix' to 'graycomatrix'\n",
        "        glcm = graycomatrix(img, [1], [0], symmetric=True, normed=True)\n",
        "        # Changed 'greycoprops' to 'graycoprops'\n",
        "        contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
        "        energy = graycoprops(glcm, 'energy')[0, 0]\n",
        "        features.append([contrast, energy])\n",
        "    return np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pgfvWc4HpQ-"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "images = generate_synthetic_images()\n",
        "stats = compute_statistics(images)\n",
        "texture_features = extract_texture_features(images)\n",
        "all_features = np.hstack((stats, texture_features))\n",
        "# Save the features to 'features.npy'\n",
        "np.save(\"features.npy\", all_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "yK1Q6wGMHpQ-",
        "outputId": "7eb75df0-8c9b-4ccb-d357-cc1a4c1dd318"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualizations\n",
        "sns.heatmap(np.corrcoef(all_features.T), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "visualize_dim_reduction(all_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBoGr-G2HpQ-"
      },
      "outputs": [],
      "source": [
        "#Phase 3\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX2gUcsfHpQ-",
        "outputId": "82f7a252-bd02-43d8-e165-eec1e8ca1f04"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "if os.path.exists(\"features.npy\") and os.path.exists(\"labels.npy\"):\n",
        "    X_features = np.load(\"features.npy\")\n",
        "    y_labels = np.load(\"labels.npy\")\n",
        "    print(\"✅ Features and labels loaded successfully.\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"features.npy or labels.npy not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuvxMvbGHpQ_"
      },
      "outputs": [],
      "source": [
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql3VXqHkHpQ_",
        "outputId": "52eb81bb-538c-4514-9319-9de79de0a2e5"
      },
      "outputs": [],
      "source": [
        "# --- Implement baseline models (Random Forest, SVM, XGBoost) ---\n",
        "print(\"\\n--- Baseline Model Training ---\")\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"SVM\": SVC(kernel='linear', probability=True),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}:\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "    print(f\"{name} Accuracy: {acc:.4f}, AUC-ROC: {auc:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuA9kEyPHpQ_",
        "outputId": "522b23e9-28f5-4250-9ac2-f047042683d0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/University /Spring 2025/Data Mining and Machine Learning /Project/GAN-Traning Images\"\n",
        "\n",
        "# Check if the dataset path exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"✅ Dataset path exists: {dataset_path}\")\n",
        "else:\n",
        "    print(f\"❌ Dataset path does NOT exist: {dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfdHePoEHpQ_",
        "outputId": "222b1583-3bb4-4613-ac6c-31a5e539800f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# List the contents of the dataset folder\n",
        "print(\"Contents of dataset folder:\")\n",
        "print(os.listdir(dataset_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgVpA4tjHpQ_",
        "outputId": "3162001c-fed8-4186-e209-d609b6674886"
      },
      "outputs": [],
      "source": [
        "# (Optional) If you expect subdirectories for classes, check for them\n",
        "subdirectories = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "print(\"Detected subdirectories:\", subdirectories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtaxbHePHpQ_",
        "outputId": "205ded08-36cd-4ed9-935c-7431a9088031"
      },
      "outputs": [],
      "source": [
        "# (Optional) If you expect image files directly in the dataset path (not recommended for flow_from_directory), list a few\n",
        "image_files = [f for f in os.listdir(dataset_path) if f.endswith(('.jpg', '.jpeg', '.png'))][:10]\n",
        "print(\"Sample image files (if any) in the main directory:\", image_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwsIfugtHpQ_",
        "outputId": "ba5effef-0e9b-4ecc-c441-0eccfc7cdbd3"
      },
      "outputs": [],
      "source": [
        "# --- Deep Learning models (CNNs like EfficientNet, ResNet, Transformer-based models) ---\n",
        "print(\"\\n--- Deep Learning Model Training (ResNet50) ---\")\n",
        "dataset_path = \"/content/drive/MyDrive/University /Spring 2025/Data Mining and Machine Learning /Project/GAN-Traning Images\" # Update with actual path\n",
        "\n",
        "# Ensure your dataset has a subdirectory structure like:\n",
        "# dataset_path/train/class1/*.jpg\n",
        "# dataset_path/train/class2/*.jpg\n",
        "# dataset_path/validation/class1/*.jpg\n",
        "# dataset_path/validation/class2/*.jpg\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    dataset_path,  # Main dataset directory\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',  # Or 'categorical' for multi-class\n",
        "    subset='training'  # Use the training subset\n",
        ")\n",
        "\n",
        "val_data = train_datagen.flow_from_directory(\n",
        "    dataset_path,  # Main dataset directory\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',  # Or 'categorical' for multi-class\n",
        "    subset='validation'  # Use the validation subset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugKgQ0xLHpQ_"
      },
      "outputs": [],
      "source": [
        "# Define CNN Model (ResNet50)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False # Freeze base layers\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_bNUA3mHpQ_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_fscore_support\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2gwdzMGHpQ_"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and display metrics\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    if hasattr(model, \"predict_proba\"):  # For ML models with probability outputs\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_pred_proba = y_pred  # For CNNs, predictions are already probabilities\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, AUC-ROC: {auc:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(f\"Confusion Matrix for {model_name}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WzZB8bMhHpQ_",
        "outputId": "24bc3c1b-af9e-4c55-85ba-1d3f99fe818e"
      },
      "outputs": [],
      "source": [
        "# --- Implement baseline models (Random Forest, SVM, XGBoost) ---\n",
        "print(\"\\n--- Baseline Model Training ---\")\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"SVM\": SVC(kernel='linear', probability=True),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Train and assign models to variables\n",
        "rf = models[\"Random Forest\"]  # Assign Random Forest model to 'rf'\n",
        "svm = models[\"SVM\"]  # Assign SVM model to 'svm'\n",
        "xgb = models[\"XGBoost\"]  # Assign XGBoost model to 'xgb'\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}:\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "    print(f\"{name} Accuracy: {acc:.4f}, AUC-ROC: {auc:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "# ... (Rest of your code) ...\n",
        "\n",
        "# Evaluate ML Models\n",
        "evaluate_model(rf, X_test, y_test, \"Random Forest\")\n",
        "evaluate_model(svm, X_test, y_test, \"SVM\")\n",
        "evaluate_model(xgb, X_test, y_test, \"XGBoost\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCNt5AsqHpQ_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def classify_risk(glcm_contrast, entropy, skewness, ml_confidence):\n",
        "    \"\"\"\n",
        "    Rule-based classification of patient risk levels.\n",
        "\n",
        "    Parameters:\n",
        "    - glcm_contrast: Measure of texture contrast from MRI\n",
        "    - entropy: Image entropy, indicating disorder\n",
        "    - skewness: Statistical measure of asymmetry in image features\n",
        "    - ml_confidence: Probability score from ML model (0 to 1)\n",
        "\n",
        "    Returns:\n",
        "    - Risk level as a string (\"Low\", \"Medium\", \"High\")\n",
        "    \"\"\"\n",
        "    if ml_confidence < 0.5:\n",
        "        return \"Low\"\n",
        "    elif glcm_contrast > 0.6 and entropy > 5.0:\n",
        "        return \"High\"\n",
        "    elif skewness < -0.5 or skewness > 0.5:\n",
        "        return \"Medium\"\n",
        "    elif ml_confidence >= 0.7:\n",
        "        return \"High\"\n",
        "    else:\n",
        "        return \"Medium\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYAm4LpCHpQ_",
        "outputId": "e2e2de09-87d2-4d2b-b2eb-c1d7c761cda1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage with test data\n",
        "example_patients = [\n",
        "    (0.5, 4.2, 0.3, 0.8),  # High risk (ML confidence high)\n",
        "    (0.3, 3.8, -0.2, 0.4), # Low risk (ML confidence low)\n",
        "    (0.7, 5.5, 0.6, 0.6),  # High risk (high contrast & entropy)\n",
        "    (0.4, 4.0, -0.6, 0.6)  # Medium risk (skewness out of range)\n",
        "]\n",
        "\n",
        "for i, patient in enumerate(example_patients):\n",
        "    risk_level = classify_risk(*patient)\n",
        "    print(f\"Patient {i+1} Risk Level: {risk_level}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgMSnUUkHpQ_",
        "outputId": "3dcad7c8-2d98-4d25-8840-f9c77ae01b01"
      },
      "outputs": [],
      "source": [
        "#Phase 4\n",
        "import dash\n",
        "from dash import dcc, html, Input, Output\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import streamlit as st\n",
        "st.title(\"Test Dashboard\")\n",
        "st.write(\"If you see this, Streamlit is working!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNJBOA_yHpQ_"
      },
      "outputs": [],
      "source": [
        "# Sample MRI Data & Risk Classification (Replace with real data)\n",
        "data = pd.DataFrame({\n",
        "    'Patient ID': [101, 102, 103, 104, 105],\n",
        "    'Risk Score': [0.2, 0.6, 0.8, 0.4, 0.9],\n",
        "    'Risk Category': ['Low', 'Medium', 'High', 'Medium', 'High']\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "TQjugGTfHpRA",
        "outputId": "aa693761-046c-4da9-c954-7492ead80015"
      },
      "outputs": [],
      "source": [
        "# Convert MRI Image to Base64 (Replace with actual MRI paths)\n",
        "def encode_image(image_path):\n",
        "    if os.path.exists(image_path):\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            return \"data:image/png;base64,\" + base64.b64encode(img_file.read()).decode()\n",
        "    return None\n",
        "\n",
        "# Sample MRI Image Path (Replace with actual images)\n",
        "mri_image_path = \"sample_mri.png\"\n",
        "encoded_image = encode_image(mri_image_path)\n",
        "\n",
        "# Dash App Setup\n",
        "app = dash.Dash(__name__)\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"MRI Clinical Decision Support Dashboard\"),\n",
        "\n",
        "    # Risk Category Filter\n",
        "    html.Label(\"Filter by Risk Category:\"),\n",
        "    dcc.Dropdown(\n",
        "        id='risk-filter',\n",
        "        options=[{'label': cat, 'value': cat} for cat in data['Risk Category'].unique()],\n",
        "        value='All',\n",
        "        clearable=False\n",
        "    ),\n",
        "\n",
        "    # Risk Distribution Chart\n",
        "    dcc.Graph(id='risk-chart'),\n",
        "\n",
        "    # MRI Image Display\n",
        "    html.H3(\"MRI Scan\"),\n",
        "    html.Img(id='mri-image', src=encoded_image, style={'width': '50%'}),\n",
        "\n",
        "    # Patient Risk Table\n",
        "    html.H3(\"Patient Risk Profiles\"),\n",
        "    html.Div(id='risk-table')\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    Output('risk-chart', 'figure'),\n",
        "    Output('risk-table', 'children'),\n",
        "    Input('risk-filter', 'value')\n",
        ")\n",
        "def update_dashboard(selected_risk):\n",
        "    filtered_data = data if selected_risk == 'All' else data[data['Risk Category'] == selected_risk]\n",
        "    fig = px.histogram(filtered_data, x='Risk Category', title='Risk Category Distribution', color='Risk Category')\n",
        "\n",
        "    table = html.Table([\n",
        "        html.Tr([html.Th(col) for col in filtered_data.columns])\n",
        "    ] + [\n",
        "        html.Tr([html.Td(filtered_data.iloc[i][col]) for col in filtered_data.columns]) for i in range(len(filtered_data))\n",
        "    ])\n",
        "\n",
        "    return fig, table\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc4HzPaNHpRA"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJER60fdHpRA",
        "outputId": "9adf3e14-0ebe-49cb-e7f0-6c67c53969fc"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model\n",
        "MODEL_PATH = \"mri_model.h5\"  # Ensure the model path is correct\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model = load_model(MODEL_PATH)\n",
        "else:\n",
        "    st.error(\"Model file not found! Please upload the trained model.\")\n",
        "    st.stop()\n",
        "\n",
        "# Function to preprocess image\n",
        "def preprocess_image(image):\n",
        "    image = image.resize((224, 224))  # Resize to model's expected input size\n",
        "    image = np.array(image) / 255.0  # Normalize pixel values\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLWSfKSgHpRA",
        "outputId": "57d5320f-f321-4e6b-bdc5-d719c2348087"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Streamlit UI\n",
        "st.title(\"AI-Powered MRI Analysis System\")\n",
        "st.write(\"Upload an MRI scan to analyze and get predictions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gao5k-ZtHpRA",
        "outputId": "edd17476-4328-4542-eb07-85c70c650c30"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Upload image\n",
        "uploaded_file = st.file_uploader(\"Choose an MRI image...\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption=\"Uploaded MRI Image\", use_column_width=True)\n",
        "\n",
        "    # Preprocess and predict\n",
        "    processed_image = preprocess_image(image)\n",
        "    prediction = model.predict(processed_image)\n",
        "    probability = prediction[0][0]  # Assuming binary classification\n",
        "\n",
        "    # Display results\n",
        "    st.subheader(\"Prediction Result\")\n",
        "    if probability > 0.5:\n",
        "        st.success(f\"Positive MRI scan with {probability * 100:.2f}% confidence\")\n",
        "    else:\n",
        "        st.warning(f\"Negative MRI scan with {(1 - probability) * 100:.2f}% confidence\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMWXK1TgHnzf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYqKWpNmHJJ6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from skimage import exposure, restoration\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu3Utt50HJJ8"
      },
      "outputs": [],
      "source": [
        "# Define dataset path\n",
        "dataset_path = \"/content/drive/MyDrive/University /Spring 2025/Data Mining and Machine Learning /Project/GAN-Traning Images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUO1qsTMHJJ8",
        "outputId": "c703a6e3-0fa0-46dd-d8eb-4807d29069ad"
      },
      "outputs": [],
      "source": [
        "# Check if path exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(\"✅ Dataset path exists.\")\n",
        "    print(\"Contents of dataset folder:\", os.listdir(dataset_path))\n",
        "else:\n",
        "    print(\"❌ Dataset path does not exist. Check the path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "53q-46LVHJJ9",
        "outputId": "b3159fc4-aee2-4561-ede2-3f64daf3901f"
      },
      "outputs": [],
      "source": [
        "# Get sample images\n",
        "image_files = [f for f in os.listdir(dataset_path) if f.endswith(('.jpg', '.png'))][:6]\n",
        "image_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "3u2c6gP-HJJ9",
        "outputId": "290fd148-dcd3-4ecd-9f92-3639f86c9b77"
      },
      "outputs": [],
      "source": [
        "# Get sample images for initial visualization (grayscale)\n",
        "image_files = [f for f in os.listdir(dataset_path) if f.endswith(('.jpg', '.png'))]\n",
        "image_files\n",
        "\n",
        "# Plot initial sample images (grayscale)\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "num_images_to_plot = min(6, len(image_files)) # Ensure we don't try to plot more than 6 images\n",
        "for i in range(num_images_to_plot):\n",
        "    img_name = image_files[i]\n",
        "    img = Image.open(os.path.join(dataset_path, img_name)).convert(\"L\") # Convert to grayscale [1]\n",
        "    ax = axes[i // 3, i % 3]\n",
        "    ax.imshow(img, cmap=\"gray\")\n",
        "    ax.set_title(img_name)\n",
        "    ax.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2uP9q8mHJJ9",
        "outputId": "383eaeeb-2cce-4801-f2cb-3968ba9e8212"
      },
      "outputs": [],
      "source": [
        "# 1. Inspect dataset structure\n",
        "# We assume the dataset has a structure like: dataset_path/<class_name>/image_files...\n",
        "classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "print(\"Detected classes:\", classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXthobF9HJJ-",
        "outputId": "3f082c7d-554d-4e54-97da-37f74e86cc66"
      },
      "outputs": [],
      "source": [
        "# Inspect dataset structure and get class information\n",
        "classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "print(\"Detected classes:\", classes)\n",
        "# Get list of image paths and their labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Assuming your dataset is structured like: dataset_path/class_name/image.jpg [5, 6]\n",
        "for cls in classes:\n",
        "    cls_folder = os.path.join(dataset_path, cls)\n",
        "    imgs = glob(os.path.join(cls_folder, '*.[jp][pn]g')) # jpg, jpeg, png files [5]\n",
        "    image_paths.extend(imgs)\n",
        "    labels.extend([cls]*len(imgs))\n",
        "print(\"Total images loaded:\", len(image_paths))\n",
        "print(f\"Labels: {labels[:5]}\") # Print first 5 labels [7]\n",
        "print(\"Class distribution:\", Counter(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5deKZBNBHJJ-",
        "outputId": "0560beb0-3cdf-4ba2-b313-c1ed48679b54"
      },
      "outputs": [],
      "source": [
        "# Get list of image paths and their labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "for cls in classes:\n",
        "    cls_folder = os.path.join(dataset_path, cls)\n",
        "    # Assuming common image file extensions; adjust if necessary.\n",
        "    imgs = glob(os.path.join(cls_folder, '*.[jp][pn]g'))  # jpg, jpeg, png files\n",
        "    image_paths.extend(imgs)\n",
        "    labels.extend([cls]*len(imgs))\n",
        "\n",
        "print(\"Total images loaded:\", len(image_paths))\n",
        "print(\"Class distribution:\", Counter(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPwlZlHHHJJ-",
        "outputId": "99bc4002-e696-460c-8a65-7182494da83a"
      },
      "outputs": [],
      "source": [
        "# List all image files (assuming jpg and png images)\n",
        "image_paths = glob(os.path.join(dataset_path, \"*.[jp][pn]g\"))\n",
        "print(\"Total images found:\", len(image_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAo3K1BsHJJ-"
      },
      "outputs": [],
      "source": [
        "# 2. Load and visualize sample images\n",
        "def load_image(image_path):\n",
        "    # Read the image in color first (for visualization)\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error loading {image_path}\")\n",
        "    # Convert from BGR (OpenCV default) to RGB\n",
        "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3ytbK9zHJJ-"
      },
      "outputs": [],
      "source": [
        "# Function to load images in color (BGR to RGB conversion for visualization)\n",
        "def load_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error loading {image_path}\")\n",
        "        return None\n",
        "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hcHouRWTHJJ-",
        "outputId": "d17c296d-1382-4206-8445-0c6ca51953fd"
      },
      "outputs": [],
      "source": [
        "# Visualize a few sample images\n",
        "num_samples = 6  # Number of images to display (you can adjust this)\n",
        "\n",
        "if len(image_paths) > 0:\n",
        "    # Create subplots (adjusting layout based on number of samples)\n",
        "    cols = 3\n",
        "    rows = (num_samples + cols - 1) // cols  # Calculate required rows\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
        "\n",
        "    # Flatten axes for easy iteration if it's a 2D array\n",
        "    axes = axes.flatten() if isinstance(axes, (list, np.ndarray)) else [axes]\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if i < len(image_paths):\n",
        "            img = load_image(image_paths[i])\n",
        "            if img is not None:\n",
        "                axes[i].imshow(img)\n",
        "                axes[i].axis('off')\n",
        "                axes[i].set_title(f\"Image {i+1}\")\n",
        "        else:\n",
        "            # Hide unused subplots\n",
        "            axes[i].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No images found in the dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Cj8XohBHJJ-"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function\n",
        "def preprocess_image(image_path):\n",
        "    # Load image in grayscale\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    # Check if image is loaded\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Image at {image_path} cannot be loaded.\")\n",
        "    # Intensity normalization (contrast stretching) [12]\n",
        "    p2, p98 = np.percentile(img, (2, 98))\n",
        "    img_norm = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
        "    # Noise reduction using Gaussian filtering [12]\n",
        "    img_denoised = cv2.GaussianBlur(img_norm, (5, 5), 0)\n",
        "    # ROI extraction (placeholder) [12]\n",
        "    roi = img_denoised\n",
        "    return roi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ytL6ygYHJJ-"
      },
      "outputs": [],
      "source": [
        "# 3. Preprocessing functions\n",
        "def preprocess_image(image_path):\n",
        "    # Load image in grayscale\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if image is loaded\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Image at {image_path} cannot be loaded.\")\n",
        "\n",
        "    # Intensity normalization (contrast stretching)\n",
        "    p2, p98 = np.percentile(img, (2, 98))\n",
        "    img_norm = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
        "\n",
        "    # Noise reduction using Gaussian filtering\n",
        "    img_denoised = cv2.GaussianBlur(img_norm, (5, 5), 0)\n",
        "\n",
        "    # ROI extraction: as an example, here we simply return the full image.\n",
        "    # You might add additional ROI extraction logic based on your application.\n",
        "    roi = img_denoised\n",
        "\n",
        "    return roi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "N44MtbE4HJJ_",
        "outputId": "01ffc459-7296-4717-9826-f6c3b3b53d76"
      },
      "outputs": [],
      "source": [
        "# Process a sample image and visualize before & after preprocessing\n",
        "if image_paths:  # Check if image_paths is not empty\n",
        "    sample_path = image_paths[0]\n",
        "    original_img = cv2.imread(sample_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if original_img is not None:\n",
        "        processed_img = preprocess_image(sample_path)  # Assuming you have defined this function\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        axes[0].imshow(original_img, cmap='gray')\n",
        "        axes[0].set_title(\"Original (Grayscale)\")\n",
        "        axes[0].axis('off')\n",
        "        axes[1].imshow(processed_img, cmap='gray')\n",
        "        axes[1].set_title(\"Preprocessed\")\n",
        "        axes[1].axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Error: Could not load image at {sample_path}\")\n",
        "else:\n",
        "    print(\"No images found in the dataset. Skipping preprocessing visualization.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "GuzijYV5HJJ_",
        "outputId": "0709ea2b-23c9-4625-e213-5b235a3d90a0"
      },
      "outputs": [],
      "source": [
        "# Process a sample image and visualize before & after preprocessing\n",
        "if image_paths:  # Check if image_paths is not empty\n",
        "    sample_path = image_paths[0]\n",
        "    original_img = cv2.imread(sample_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Check if the image was loaded successfully\n",
        "    if original_img is not None:\n",
        "        processed_img = preprocess_image(sample_path)  # Assuming you have defined this function\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        axes[0].imshow(original_img, cmap='gray')\n",
        "        axes[0].set_title(\"Original (Grayscale)\")\n",
        "        axes[0].axis('off')\n",
        "        axes[1].imshow(processed_img, cmap='gray')\n",
        "        axes[1].set_title(\"Preprocessed\")\n",
        "        axes[1].axis('off')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Error: Could not load image at {sample_path}\")\n",
        "else:\n",
        "    print(\"No images found in the dataset. Skipping preprocessing visualization.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-kJkJHHHJJ_"
      },
      "outputs": [],
      "source": [
        "#Phase 2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from skimage.feature import graycomatrix, graycoprops # Changed 'greycomatrix' to 'graycomatrix' and 'greycoprops' to 'graycoprops'\n",
        "from skimage.measure import shannon_entropy\n",
        "from scipy.stats import skew, kurtosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm_3PY3ZHJJ_"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic grayscale image data (simulating MRI images)\n",
        "def generate_synthetic_images(num_images=100, img_size=(128, 128)):\n",
        "    return [np.random.randint(0, 256, img_size, dtype=np.uint8) for _ in range(num_images)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E_rh3ZEHJKA"
      },
      "outputs": [],
      "source": [
        "# Statistical Analysis\n",
        "def compute_statistics(images):\n",
        "    stats = []\n",
        "    for img in images:\n",
        "        mean_val = np.mean(img)\n",
        "        var_val = np.var(img)\n",
        "        entropy = shannon_entropy(img)\n",
        "        skewness = skew(img.flatten())\n",
        "        kurt = kurtosis(img.flatten())\n",
        "        stats.append([mean_val, var_val, entropy, skewness, kurt])\n",
        "    return np.array(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx9X0bPrHJKA"
      },
      "outputs": [],
      "source": [
        "# Visualization - PCA & t-SNE\n",
        "def visualize_dim_reduction(features):\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_pca = pca.fit_transform(features)\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    reduced_tsne = tsne.fit_transform(features)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    axes[0].scatter(reduced_pca[:, 0], reduced_pca[:, 1], alpha=0.7)\n",
        "    axes[0].set_title(\"PCA Projection\")\n",
        "\n",
        "    axes[1].scatter(reduced_tsne[:, 0], reduced_tsne[:, 1], alpha=0.7)\n",
        "    axes[1].set_title(\"t-SNE Projection\")\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvrd2pSXHJKA"
      },
      "outputs": [],
      "source": [
        "# Feature Extraction\n",
        "def extract_texture_features(images):\n",
        "    features = []\n",
        "    for img in images:\n",
        "        # Changed 'greycomatrix' to 'graycomatrix'\n",
        "        glcm = graycomatrix(img, [1], [0], symmetric=True, normed=True)\n",
        "        # Changed 'greycoprops' to 'graycoprops'\n",
        "        contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
        "        energy = graycoprops(glcm, 'energy')[0, 0]\n",
        "        features.append([contrast, energy])\n",
        "    return np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwX2yMu5HJKA"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "images = generate_synthetic_images()\n",
        "stats = compute_statistics(images)\n",
        "texture_features = extract_texture_features(images)\n",
        "all_features = np.hstack((stats, texture_features))\n",
        "# Save the features to 'features.npy'\n",
        "np.save(\"features.npy\", all_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "g2dlDYK6HJKA",
        "outputId": "8b8eb3ef-d427-4e2e-ce60-c75e95710c19"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualizations\n",
        "sns.heatmap(np.corrcoef(all_features.T), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "visualize_dim_reduction(all_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxvhURsxHJKA"
      },
      "outputs": [],
      "source": [
        "#Phase 3\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Grq5t1WHJKA",
        "outputId": "9522edba-0535-4919-beb3-a98f9d13d754"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "if os.path.exists(\"features.npy\") and os.path.exists(\"labels.npy\"):\n",
        "    X_features = np.load(\"features.npy\")\n",
        "    y_labels = np.load(\"labels.npy\")\n",
        "    print(\"✅ Features and labels loaded successfully.\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"features.npy or labels.npy not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62UECSjCHJKA"
      },
      "outputs": [],
      "source": [
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaF-baOAHJKB",
        "outputId": "b6b2ea9f-93fd-4fc3-efd4-58d36a19f9df"
      },
      "outputs": [],
      "source": [
        "# --- Implement baseline models (Random Forest, SVM, XGBoost) ---\n",
        "print(\"\\n--- Baseline Model Training ---\")\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"SVM\": SVC(kernel='linear', probability=True),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}:\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "    print(f\"{name} Accuracy: {acc:.4f}, AUC-ROC: {auc:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRkWAk7MHJKB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/University /Spring 2025/Data Mining and Machine Learning /Project/GAN-Traning Images\"\n",
        "\n",
        "# Check if the dataset path exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"✅ Dataset path exists: {dataset_path}\")\n",
        "else:\n",
        "    print(f\"❌ Dataset path does NOT exist: {dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bfrIprGHJKB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# List the contents of the dataset folder\n",
        "print(\"Contents of dataset folder:\")\n",
        "print(os.listdir(dataset_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2jSiGeeHJKB"
      },
      "outputs": [],
      "source": [
        "# (Optional) If you expect subdirectories for classes, check for them\n",
        "subdirectories = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "print(\"Detected subdirectories:\", subdirectories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLTljkP5HJKB"
      },
      "outputs": [],
      "source": [
        "# (Optional) If you expect image files directly in the dataset path (not recommended for flow_from_directory), list a few\n",
        "image_files = [f for f in os.listdir(dataset_path) if f.endswith(('.jpg', '.jpeg', '.png'))][:10]\n",
        "print(\"Sample image files (if any) in the main directory:\", image_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ_e7BX6HJKB"
      },
      "outputs": [],
      "source": [
        "# --- Deep Learning models (CNNs like EfficientNet, ResNet, Transformer-based models) ---\n",
        "print(\"\\n--- Deep Learning Model Training (ResNet50) ---\")\n",
        "dataset_path = \"/content/drive/MyDrive/University /Spring 2025/Data Mining and Machine Learning /Project/GAN-Traning Images\" # Update with actual path\n",
        "\n",
        "# Ensure your dataset has a subdirectory structure like:\n",
        "# dataset_path/train/class1/*.jpg\n",
        "# dataset_path/train/class2/*.jpg\n",
        "# dataset_path/validation/class1/*.jpg\n",
        "# dataset_path/validation/class2/*.jpg\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    dataset_path,  # Main dataset directory\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',  # Or 'categorical' for multi-class\n",
        "    subset='training'  # Use the training subset\n",
        ")\n",
        "\n",
        "val_data = train_datagen.flow_from_directory(\n",
        "    dataset_path,  # Main dataset directory\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',  # Or 'categorical' for multi-class\n",
        "    subset='validation'  # Use the validation subset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCKrCtJxHJKB"
      },
      "outputs": [],
      "source": [
        "# Define CNN Model (ResNet50)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False # Freeze base layers\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ66wdhuHJKB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_fscore_support\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TCmXD44HJKB"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and display metrics\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    if hasattr(model, \"predict_proba\"):  # For ML models with probability outputs\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_pred_proba = y_pred  # For CNNs, predictions are already probabilities\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, AUC-ROC: {auc:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(f\"Confusion Matrix for {model_name}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzg3kxxyHJKB"
      },
      "outputs": [],
      "source": [
        "# Evaluate ML Models\n",
        "evaluate_model(rf, X_test, y_test, \"Random Forest\")\n",
        "evaluate_model(svm, X_test, y_test, \"SVM\")\n",
        "evaluate_model(xgb, X_test, y_test, \"XGBoost\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIRxUZ1JHJKB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def classify_risk(glcm_contrast, entropy, skewness, ml_confidence):\n",
        "    \"\"\"\n",
        "    Rule-based classification of patient risk levels.\n",
        "\n",
        "    Parameters:\n",
        "    - glcm_contrast: Measure of texture contrast from MRI\n",
        "    - entropy: Image entropy, indicating disorder\n",
        "    - skewness: Statistical measure of asymmetry in image features\n",
        "    - ml_confidence: Probability score from ML model (0 to 1)\n",
        "\n",
        "    Returns:\n",
        "    - Risk level as a string (\"Low\", \"Medium\", \"High\")\n",
        "    \"\"\"\n",
        "    if ml_confidence < 0.5:\n",
        "        return \"Low\"\n",
        "    elif glcm_contrast > 0.6 and entropy > 5.0:\n",
        "        return \"High\"\n",
        "    elif skewness < -0.5 or skewness > 0.5:\n",
        "        return \"Medium\"\n",
        "    elif ml_confidence >= 0.7:\n",
        "        return \"High\"\n",
        "    else:\n",
        "        return \"Medium\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnlgvx9OHJKC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage with test data\n",
        "example_patients = [\n",
        "    (0.5, 4.2, 0.3, 0.8),  # High risk (ML confidence high)\n",
        "    (0.3, 3.8, -0.2, 0.4), # Low risk (ML confidence low)\n",
        "    (0.7, 5.5, 0.6, 0.6),  # High risk (high contrast & entropy)\n",
        "    (0.4, 4.0, -0.6, 0.6)  # Medium risk (skewness out of range)\n",
        "]\n",
        "\n",
        "for i, patient in enumerate(example_patients):\n",
        "    risk_level = classify_risk(*patient)\n",
        "    print(f\"Patient {i+1} Risk Level: {risk_level}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQR1a3-FHJKC"
      },
      "outputs": [],
      "source": [
        "#Phase 4\n",
        "import dash\n",
        "from dash import dcc, html, Input, Output\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import streamlit as st\n",
        "st.title(\"Test Dashboard\")\n",
        "st.write(\"If you see this, Streamlit is working!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwAXpM5LHJKC"
      },
      "outputs": [],
      "source": [
        "# Sample MRI Data & Risk Classification (Replace with real data)\n",
        "data = pd.DataFrame({\n",
        "    'Patient ID': [101, 102, 103, 104, 105],\n",
        "    'Risk Score': [0.2, 0.6, 0.8, 0.4, 0.9],\n",
        "    'Risk Category': ['Low', 'Medium', 'High', 'Medium', 'High']\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be6jmhCkHJKC"
      },
      "outputs": [],
      "source": [
        "# Convert MRI Image to Base64 (Replace with actual MRI paths)\n",
        "def encode_image(image_path):\n",
        "    if os.path.exists(image_path):\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            return \"data:image/png;base64,\" + base64.b64encode(img_file.read()).decode()\n",
        "    return None\n",
        "\n",
        "# Sample MRI Image Path (Replace with actual images)\n",
        "mri_image_path = \"sample_mri.png\"\n",
        "encoded_image = encode_image(mri_image_path)\n",
        "\n",
        "# Dash App Setup\n",
        "app = dash.Dash(__name__)\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"MRI Clinical Decision Support Dashboard\"),\n",
        "\n",
        "    # Risk Category Filter\n",
        "    html.Label(\"Filter by Risk Category:\"),\n",
        "    dcc.Dropdown(\n",
        "        id='risk-filter',\n",
        "        options=[{'label': cat, 'value': cat} for cat in data['Risk Category'].unique()],\n",
        "        value='All',\n",
        "        clearable=False\n",
        "    ),\n",
        "\n",
        "    # Risk Distribution Chart\n",
        "    dcc.Graph(id='risk-chart'),\n",
        "\n",
        "    # MRI Image Display\n",
        "    html.H3(\"MRI Scan\"),\n",
        "    html.Img(id='mri-image', src=encoded_image, style={'width': '50%'}),\n",
        "\n",
        "    # Patient Risk Table\n",
        "    html.H3(\"Patient Risk Profiles\"),\n",
        "    html.Div(id='risk-table')\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    Output('risk-chart', 'figure'),\n",
        "    Output('risk-table', 'children'),\n",
        "    Input('risk-filter', 'value')\n",
        ")\n",
        "def update_dashboard(selected_risk):\n",
        "    filtered_data = data if selected_risk == 'All' else data[data['Risk Category'] == selected_risk]\n",
        "    fig = px.histogram(filtered_data, x='Risk Category', title='Risk Category Distribution', color='Risk Category')\n",
        "\n",
        "    table = html.Table([\n",
        "        html.Tr([html.Th(col) for col in filtered_data.columns])\n",
        "    ] + [\n",
        "        html.Tr([html.Td(filtered_data.iloc[i][col]) for col in filtered_data.columns]) for i in range(len(filtered_data))\n",
        "    ])\n",
        "\n",
        "    return fig, table\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63wsuz9lHJKC"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnNalQKgHJKC"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained model\n",
        "MODEL_PATH = \"mri_model.h5\"  # Ensure the model path is correct\n",
        "if os.path.exists(MODEL_PATH):\n",
        "    model = load_model(MODEL_PATH)\n",
        "else:\n",
        "    st.error(\"Model file not found! Please upload the trained model.\")\n",
        "    st.stop()\n",
        "\n",
        "# Function to preprocess image\n",
        "def preprocess_image(image):\n",
        "    image = image.resize((224, 224))  # Resize to model's expected input size\n",
        "    image = np.array(image) / 255.0  # Normalize pixel values\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADoi0ewLHJKC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Streamlit UI\n",
        "st.title(\"AI-Powered MRI Analysis System\")\n",
        "st.write(\"Upload an MRI scan to analyze and get predictions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swM8jZ4bHJKC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Upload image\n",
        "uploaded_file = st.file_uploader(\"Choose an MRI image...\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    image = Image.open(uploaded_file)\n",
        "    st.image(image, caption=\"Uploaded MRI Image\", use_column_width=True)\n",
        "\n",
        "    # Preprocess and predict\n",
        "    processed_image = preprocess_image(image)\n",
        "    prediction = model.predict(processed_image)\n",
        "    probability = prediction[0][0]  # Assuming binary classification\n",
        "\n",
        "    # Display results\n",
        "    st.subheader(\"Prediction Result\")\n",
        "    if probability > 0.5:\n",
        "        st.success(f\"Positive MRI scan with {probability * 100:.2f}% confidence\")\n",
        "    else:\n",
        "        st.warning(f\"Negative MRI scan with {(1 - probability) * 100:.2f}% confidence\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmlPSXy78FO_"
      },
      "outputs": [],
      "source": [
        "# --- Implement baseline models (Random Forest, SVM, XGBoost) ---\n",
        "print(\"\\n--- Baseline Model Training ---\")\n",
        "models = {\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"SVM\": SVC(kernel='linear', probability=True),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}:\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
        "    print(f\"{name} Accuracy: {acc:.4f}, AUC-ROC: {auc:.4f}\")\n",
        "    print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnjYTp7u8_5U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/University /Spring 2025/Data Mining and Machine Learning /Project/GAN-Traning Images\"\n",
        "\n",
        "# Check if the dataset path exists\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"✅ Dataset path exists: {dataset_path}\")\n",
        "else:\n",
        "    print(f\"❌ Dataset path does NOT exist: {dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0JIGR1H9FEj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# List the contents of the dataset folder\n",
        "print(\"Contents of dataset folder:\")\n",
        "print(os.listdir(dataset_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t3twA0U9FHB"
      },
      "outputs": [],
      "source": [
        "# (Optional) If you expect subdirectories for classes, check for them\n",
        "subdirectories = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "print(\"Detected subdirectories:\", subdirectories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3uo2UtF9FJQ"
      },
      "outputs": [],
      "source": [
        "# (Optional) If you expect image files directly in the dataset path (not recommended for flow_from_directory), list a few\n",
        "image_files = [f for f in os.listdir(dataset_path) if f.endswith(('.jpg', '.jpeg', '.png'))][:10]\n",
        "print(\"Sample image files (if any) in the main directory:\", image_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AIiJt8H8FR7"
      },
      "outputs": [],
      "source": [
        "# --- Deep Learning models (CNNs like EfficientNet, ResNet, Transformer-based models) ---\n",
        "print(\"\\n--- Deep Learning Model Training (ResNet50) ---\")\n",
        "dataset_path = \"/content/drive/MyDrive/University /Spring 2025/Data Mining and Machine Learning /Project/GAN-Traning Images\" # Update with actual path\n",
        "\n",
        "# Ensure your dataset has a subdirectory structure like:\n",
        "# dataset_path/train/class1/*.jpg\n",
        "# dataset_path/train/class2/*.jpg\n",
        "# dataset_path/validation/class1/*.jpg\n",
        "# dataset_path/validation/class2/*.jpg\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    dataset_path,  # Main dataset directory\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',  # Or 'categorical' for multi-class\n",
        "    subset='training'  # Use the training subset\n",
        ")\n",
        "\n",
        "val_data = train_datagen.flow_from_directory(\n",
        "    dataset_path,  # Main dataset directory\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',  # Or 'categorical' for multi-class\n",
        "    subset='validation'  # Use the validation subset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIy1LQNE8FUf"
      },
      "outputs": [],
      "source": [
        "# Define CNN Model (ResNet50)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False # Freeze base layers\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TQ4i67z8FZp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, precision_recall_fscore_support\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EXg8ZkR8FcX"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and display metrics\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    if hasattr(model, \"predict_proba\"):  # For ML models with probability outputs\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        y_pred_proba = y_pred  # For CNNs, predictions are already probabilities\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, AUC-ROC: {auc:.4f}\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Class 0\", \"Class 1\"], yticklabels=[\"Class 0\", \"Class 1\"])\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.title(f\"Confusion Matrix for {model_name}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwQ5DBg_8FfD"
      },
      "outputs": [],
      "source": [
        "# Evaluate ML Models\n",
        "evaluate_model(rf, X_test, y_test, \"Random Forest\")\n",
        "evaluate_model(svm, X_test, y_test, \"SVM\")\n",
        "evaluate_model(xgb, X_test, y_test, \"XGBoost\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqFk3YtpCdSU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def classify_risk(glcm_contrast, entropy, skewness, ml_confidence):\n",
        "    \"\"\"\n",
        "    Rule-based classification of patient risk levels.\n",
        "\n",
        "    Parameters:\n",
        "    - glcm_contrast: Measure of texture contrast from MRI\n",
        "    - entropy: Image entropy, indicating disorder\n",
        "    - skewness: Statistical measure of asymmetry in image features\n",
        "    - ml_confidence: Probability score from ML model (0 to 1)\n",
        "\n",
        "    Returns:\n",
        "    - Risk level as a string (\"Low\", \"Medium\", \"High\")\n",
        "    \"\"\"\n",
        "    if ml_confidence < 0.5:\n",
        "        return \"Low\"\n",
        "    elif glcm_contrast > 0.6 and entropy > 5.0:\n",
        "        return \"High\"\n",
        "    elif skewness < -0.5 or skewness > 0.5:\n",
        "        return \"Medium\"\n",
        "    elif ml_confidence >= 0.7:\n",
        "        return \"High\"\n",
        "    else:\n",
        "        return \"Medium\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O0tVYdtCd8r"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example usage with test data\n",
        "example_patients = [\n",
        "    (0.5, 4.2, 0.3, 0.8),  # High risk (ML confidence high)\n",
        "    (0.3, 3.8, -0.2, 0.4), # Low risk (ML confidence low)\n",
        "    (0.7, 5.5, 0.6, 0.6),  # High risk (high contrast & entropy)\n",
        "    (0.4, 4.0, -0.6, 0.6)  # Medium risk (skewness out of range)\n",
        "]\n",
        "\n",
        "for i, patient in enumerate(example_patients):\n",
        "    risk_level = classify_risk(*patient)\n",
        "    print(f\"Patient {i+1} Risk Level: {risk_level}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6SraW_QCd-1"
      },
      "outputs": [],
      "source": [
        "#Phase 4\n",
        "import dash\n",
        "from dash import dcc, html, Input, Output\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import streamlit as st\n",
        "st.title(\"Test Dashboard\")\n",
        "st.write(\"If you see this, Streamlit is working!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IVD-CzFCeBF"
      },
      "outputs": [],
      "source": [
        "# Sample MRI Data & Risk Classification (Replace with real data)\n",
        "data = pd.DataFrame({\n",
        "    'Patient ID': [101, 102, 103, 104, 105],\n",
        "    'Risk Score': [0.2, 0.6, 0.8, 0.4, 0.9],\n",
        "    'Risk Category': ['Low', 'Medium', 'High', 'Medium', 'High']\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws3joPWTCeDX"
      },
      "outputs": [],
      "source": [
        "# Convert MRI Image to Base64 (Replace with actual MRI paths)\n",
        "def encode_image(image_path):\n",
        "    if os.path.exists(image_path):\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            return \"data:image/png;base64,\" + base64.b64encode(img_file.read()).decode()\n",
        "    return None\n",
        "\n",
        "# Sample MRI Image Path (Replace with actual images)\n",
        "mri_image_path = \"sample_mri.png\"\n",
        "encoded_image = encode_image(mri_image_path)\n",
        "\n",
        "# Dash App Setup\n",
        "app = dash.Dash(__name__)\n",
        "app.layout = html.Div([\n",
        "    html.H1(\"MRI Clinical Decision Support Dashboard\"),\n",
        "\n",
        "    # Risk Category Filter\n",
        "    html.Label(\"Filter by Risk Category:\"),\n",
        "    dcc.Dropdown(\n",
        "        id='risk-filter',\n",
        "        options=[{'label': cat, 'value': cat} for cat in data['Risk Category'].unique()],\n",
        "        value='All',\n",
        "        clearable=False\n",
        "    ),\n",
        "\n",
        "    # Risk Distribution Chart\n",
        "    dcc.Graph(id='risk-chart'),\n",
        "\n",
        "    # MRI Image Display\n",
        "    html.H3(\"MRI Scan\"),\n",
        "    html.Img(id='mri-image', src=encoded_image, style={'width': '50%'}),\n",
        "\n",
        "    # Patient Risk Table\n",
        "    html.H3(\"Patient Risk Profiles\"),\n",
        "    html.Div(id='risk-table')\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    Output('risk-chart', 'figure'),\n",
        "    Output('risk-table', 'children'),\n",
        "    Input('risk-filter', 'value')\n",
        ")\n",
        "def update_dashboard(selected_risk):\n",
        "    filtered_data = data if selected_risk == 'All' else data[data['Risk Category'] == selected_risk]\n",
        "    fig = px.histogram(filtered_data, x='Risk Category', title='Risk Category Distribution', color='Risk Category')\n",
        "\n",
        "    table = html.Table([\n",
        "        html.Tr([html.Th(col) for col in filtered_data.columns])\n",
        "    ] + [\n",
        "        html.Tr([html.Td(filtered_data.iloc[i][col]) for col in filtered_data.columns]) for i in range(len(filtered_data))\n",
        "    ])\n",
        "\n",
        "    return fig, table\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
